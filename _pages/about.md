---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span class='anchor' id='about-me'></span>

I am Jian Lang, currently a master candidate in Software Engineering at the University of Electronic Science and Technology of China (UESTC), under the supervision of Prof [Fan Zhou](https://scholar.google.com/citations?user=Ihj2Rw8AAAAJ). Before that, I received my Bachelor of Engineering degree from Fuzhou University. 
  
My research mainly focuses on <strong>robust, reliable, and stable multimodal systems</strong> that can perform effectively under <strong>imperfect multimodal data</strong>, especially in the situations like <strong>missing modalities, distribution (domain) shifts, weak supervision (label scarcity)</strong>. And I am also interested in <strong>video analysis, detection</strong>, and <strong>large multimodal models</strong> for some applications.
  
Feel free to contact me if you have any questions about my research or potential collaboration opportunities.


# üî• News
- *2025.11*: &nbsp;üéâüéâ 3 Papers are accepted by KDD 2026! See you in Jeju!
- *2025.11*: &nbsp;üí¶üí¶ 3 Papers are submitted to CVPR 2026. Hope a wonderful result.
- *2025.10*: &nbsp;üí¶üí¶ 1 Paper is submitted to WWW 2026. Hope a wonderful result.
- *2025.10*: &nbsp;üéâüéâ Get Postgraduate National Scholarship again.



# üìù Selected Publications (\*=Equal Contribution, ‚Ä†=Conresponding Author)
## üõ° Robust Multimodal Learning



<div class='paper-box'><div class='paper-box-image'><div class="badge">KDD 2026</div><img src='images/radar.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Nip Rumors in the Bud: Retrieval-Guided Topic-Level Adaptation for Test-Time Fake News Video Detection](https)

**Jian Lang**, Rongpei Hong, Ting Zhong, Yong Wang, Fan Zhou‚Ä†

<!-- **CCF A** \|  -->

**KDD 2026** \| **CCF A** \| [**PDF**](/) \| [**Github**](https://github.com/Jian-Lang/RADAR) 

- RADAR, the first work to achieves the **test-time adaptation** of the Fake News Video Detection, 
- Enabling fast adaptation to evolving news videos with **shifting topic-level distributions** in the dynamic world.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div class="badge">KDD 2026</div><img src='images/alarm.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement](https) 

**Jian Lang**, Rongpei Hong, Ting Zhong, Leiting Chen, Qiang Gao, Fan Zhou‚Ä†

<!-- **CCF A** \| [**Github**](https://github.com/Jian-Lang/ALARM)  -->

**KDD 2026** \| **CCF A** \| [**PDF**](/) \| [**Github**](https://github.com/Jian-Lang/ALARM) 

- ALARM, the first label-free harmful meme detection framework powered by LMM self-improvement
- Enabling **prompt and robust adaptation** to **evolving topics and themes** of harmful web memes.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div class="badge">AAAI 2026</div><img src='images/scanner.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Shedding the Facades, Connecting the Domains: Detecting Shifting Multimodal Hate Video with Test-Time Adaptation](https)

Jiao Li, **Jian Lang**, Xikai Tang‚Ä†, Ting Zhong, Fan Zhou

**AAAI 2026** \| **CCF A** \| [**PDF**](/) \| [**Github**](https://github.com/) 

- SCANNER, the first **test-time adaptation** framework tailored for distribution shifting hate video detection.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div class="badge">KDD 2025</div><img src='images/redeem.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[REDEEMing Modality Information Loss: Retrieval-Guided Conditional Generation for Severely Modality Missing Learning](https://dl.acm.org/doi/10.1145/3711896.3737101)

**Jian Lang**, Rongpei Hong, Zhangtao Cheng, Ting Zhong, Fan Zhou‚Ä†

**KDD 2025** \| **CCF A** \| [**PDF**](/papers/REDEEM.pdf) \| [**Github**](https://github.com/Jian-Lang/REDEEM) 

- REDEEM, the extension work of our RAGPT.
- Proposing a **retrieval-guided conditional generation** paradigm for enhancing the **modality-missing robustness** of pre-trained Multimodal Transformer.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div class="badge">AAAI 2025</div><img src='images/ragpt.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Retrieval-Augmented Dynamic Prompt Tuning for Incomplete Multimodal Learning](https://doi.org/10.1609/aaai.v39i17.33984)

**Jian Lang**\*, Zhangtao Cheng\*, Ting Zhong, Fan Zhou‚Ä†

**AAAI 2025** \| **CCF A** \| [**PDF**](/papers/RAGPT.pdf) \| [**Github**](https://github.com/Jian-Lang/RAGPT) \|
[![](https://img.shields.io/github/stars/Jian-Lang/RAGPT?style=social&label=RAGPT%20Stars)](https://github.com/Jian-Lang/RAGPT) 
<!-- \| [**Python Package**](https://imml.readthedocs.io/stable/main/modules/classify.html) -->


- RAGPT, a novel **retrieval-augmented dynamic prompt-tuning** framework for enhancing the **modality-missing robustness** of pre-trained Multimodal Transformer.
</div>
</div>



<!-- <div class='paper-box'><div class='paper-box-image'><img src='images/fs.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[FastSpeech: Fast, Robust and Controllable Text to Speech](https://papers.nips.cc/paper/8580-fastspeech-fast-robust-and-controllable-text-to-speech.pdf), **Yi Ren**, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu

**NeurIPS 2019** \| [**Github**](https://speechresearch.github.io/fastspeech/) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>

- FastSpeech is the first fully parallel end-to-end speech synthesis model.
- **Academic Impact**: This work is included by many famous speech synthesis open-source projects, such as [ESPNet ![](https://img.shields.io/github/stars/espnet/espnet?style=social)](https://github.com/espnet/espnet). Our work are promoted by more than 20 media and forums, such as [Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s/UkFadiUBy-Ymn-zhJ95JcQ)„ÄÅ[InfoQ](https://www.infoq.cn/article/tvy7hnin8bjvlm6g0myu).
- **Industry Impact**: FastSpeech has been deployed in [Microsoft Azure TTS service](https://techcommunity.microsoft.com/t5/azure-ai/neural-text-to-speech-extends-support-to-15-more-languages-with/ba-p/1505911) and supports 49 more languages with state-of-the-art AI quality. It was also shown as a text-to-speech system acceleration example in [NVIDIA GTC2020](https://resources.nvidia.com/events/GTC2020s21420).
</div>
</div> -->

## üé• Video Analysis & Detection

<!-- <div class='paper-box'><div class='paper-box-image'><img src='images/radar.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Nipping Rumors in the Bud: Retrieval-Guided Topic Adaptation for Test-Time Detection of Fake News Videos](https), **Jian Lang**, Rongpei Hong, Ting Zhong, Yong Wang, Fan Zhou‚Ä†

**KDD 2026** \| [**Github**](https://github.com/Jian-Lang/RADAR) 

- RADAR is the first work to achieves the test-time adaptation of the Fake News Video Detection, enabling fast adaptation of obselete models to evolving news videos with shifting topic distributions in the dynamic world.
</div>
</div> -->




<!-- <div class='paper-box'><div class='paper-box-image'><img src='images/scanner.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Shedding the Facades, Connecting the Domains: Detecting Shifting Multimodal Hate Video with Test-Time Adaptation](https), Jiao Li, **Jian Lang**, Xikai Tang‚Ä†, Ting Zhong, Fan Zhou

**AAAI 2026** \| [**Github**](https://github.com/Jolieresearch/SCANNER) 

- SCANNER is the first test-time adaptation framework tailored for distribution shifting hate video detection.
</div>
</div>

 -->
<div class='paper-box'><div class='paper-box-image'> <div class="badge">ICCV 2025</div><img src='images/crave.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">
[Borrowing Eyes for the Blind Spot: Overcoming Data Scarcity in Malicious Video Detection via Cross-Domain Retrieval Augmentation](https://openaccess.thecvf.com/content/ICCV2025/html/Hong_Borrowing_Eyes_for_the_Blind_Spot_Overcoming_Data_Scarcity_in_ICCV_2025_paper.html)

Rongpei Hong\*, **Jian Lang**\*, Ting Zhong, Fan Zhou‚Ä†

**ICCV 2025** \| **CCF A** \| [**PDF**](/papers/CRAVE.pdf) \| [**Github**](https://github.com/ronpay/CRAVE)

- CRAVE, a novel **cross-domain retrieval augmentation** framework that transfers knowledge from resource-rich image-text domain to enhance malicious video detection.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'> <div class="badge">WWW 2025</div><img src='images/more.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Biting Off More Than You Can Detect: Retrieval-Augmented Multimodal Experts for Short Video Hate Detection](https://dl.acm.org/doi/10.1145/3696410.3714560)

**Jian Lang**, Rongpei Hong, Jin Xu, Xovee Xu, Yili Li, Fan Zhou‚Ä†


**WWW 2025** \| **CCF A** \| [**PDF**](/papers/MoRE.pdf) \| [**Github**](https://github.com/Jian-Lang/MoRE) 

- MoRE, a novel **mixture of retrieval-augmented multimodal experts** framework to enhance hate video detection.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div class="badge">WWW 2025</div><img src='images/exmrd.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Following Clues, Approaching the Truth: Explainable Micro-Video Rumor Detection via Chain-of-Thought Reasoning](https://dl.acm.org/doi/10.1145/3696410.3714559)

Rongpei Hong, **Jian Lang**, Jin Xu, Zhangtao Cheng, Ting Zhong‚Ä†, Fan Zhou

**WWW 2025** \| **CCF A** \| [**PDF**](/papers/ExMRD.pdf) \| [**Github**](https://github.com/ronpay/ExMRD) 

- ExMRD, the first **explainable fake news video detection** framework powered by the Chain-of-Thought Reasoning.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">ICME 2025</div><img src='images/real.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[REAL: Retrieval-Augmented Prototype Alignment for Improved Fake News Video Detection](https://doi.org/10.1109/ICME59968.2025.11209008)

Yili Li, **Jian Lang**, Rongpei Hong, Qing Chen, Zhangtao Cheng, Jia Chen, Ting Zhong, Fan Zhou‚Ä†

**ICME 2025** \| **CCF B** \| [**PDF**](/papers/REAL.pdf) \| [**Github**](https://github.com/Jian-Lang/REAL) 

- REAL, a novel model-agnostic framework that generates **manipulation-aware representations** to enhance existing methods in detecting fake news videos with only subtle modifications to the original authentic ones.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div class="badge">SIGIR 2024</div><img src='images/mmra.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Predicting Micro-video Popularity via Multi-modal Retrieval Augmentation](https://dl.acm.org/doi/10.1145/3626772.3657929)

Ting Zhong, **Jian Lang**, Yifan Zhang, Zhangtao Cheng, Kunpeng Zhang, Fan Zhou‚Ä†

**SIGIR 2024** \| **CCF A** \| [**PDF**](/papers/MMRA.pdf) \|  [**Github**](https://github.com/ICDM-UESTC/MMRA) \|
[![](https://img.shields.io/github/stars/ICDM-UESTC/MMRA?style=social&label=MMRA%20Stars)](https://github.com/ICDM-UESTC/MMRA)


- MMRA, a **multi-modal retrieval-augmented popularity prediction** model that enhances prediction accuracy using relevant retrieved information.
</div>
</div>



## üå† Large Multimodal Model Personalization


<!-- <div class='paper-box'><div class='paper-box-image'><img src='images/alarm.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement](https), **Jian Lang**, Rongpei Hong, Ting Zhong, Leiting Chen, Qiang Gao, Fan Zhou‚Ä†

**KDD 2026** \| [**Github**](https://github.com/Jian-Lang/ALARM) 

- ALARM is the first label-free harmful meme detection framework powered by Large Multimodal Model self-improvement, which mitigates label scarcity and enables prompt and robust adaptation to evolving harmful content in web memes.
</div>
</div> -->


<div class='paper-box'><div class='paper-box-image'><div class="badge">KDD 2026</div><img src='images/tame.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[TAMEing Long Contexts in Personalization: Towards Training-Free and State-Aware MLLM Personalized Assistant](https)

Rongpei Hong, **Jian Lang**, Ting Zhong‚Ä†, Yong Wang, Fan Zhou

**KDD 2026** \| **CCF A** \| [**PDF**](/) \| [**Github**](https://github.com/ronpay/TAME) 

- TAME, the first training-free and **state-aware** personalized Multimodal Large Multimodal Model assistant powered by double memories.
</div>
</div>




# üéñ Honors and Awards
- *2025.10* National Scholarship (Top 1%)
- *2025.10* Master's Student Academic Scholarship (1st Division, Ranked 1st)
- *2024.10* National Scholarship (Top 1%)
- *2024.10* Master's Student Academic Scholarship (1st Division, Ranked 1st)
- *2023.12* Artificial Intelligence Algorithm Challenge Runner-up (2nd), hosted by People's Daily Online


# üìñ Educations
- *2023.09 -*, Master, University of Electronic Science and Technology of China
- *2019.09 - 2023.06*, Undergraduate, Fuzhou University


# üíª Internships
- *2022.03 - 2022.06*, [Ruijie Networks](https://www.ruijie.com.cn/), Software Development Intern.
