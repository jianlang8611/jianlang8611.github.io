---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span class='anchor' id='about-me'></span>

I am Jian Lang, currently a master candidate in Software Engineering at the University of Electronic Science and Technology of China (UESTC), under the supervision of Prof [Fan Zhou](https://scholar.google.com/citations?user=Ihj2Rw8AAAAJ). Before that, I received my Bachelor of Engineering degree from Fuzhou University. 
  
My research mainly focuses on <strong>robust, reliable, and stable multimodal systems</strong> that can perform effectively under <strong>imperfect multimodal data</strong>, especially in the situations like missing modalities, distribution (domain) shifts, weak supervision (label scarcity), and data scarcity. And I am also interested in video analysis, detection, and large multimodal models for some applications.
  
Feel free to contact me if you have any questions about my research or potential collaboration opportunities.


# üî• News
- *2025.11*: &nbsp;üéâüéâ 3 Papers are accepted by KDD 2026! See you in Jeju!
- *2025.11*: 3 Papers are submitted to CVPR 2026. Hope a wonderful result.
- *2025.10*: &nbsp;üéâüéâ Get Postgraduate National Scholarship Again.



# üìù Publications 
## Robust Multimodal Learning



<div class='paper-box'><div class='paper-box-image'><div class="badge">KDD 2026</div><div class="badge">CCF A</div><img src='images/radar.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Nipping Rumors in the Bud: Retrieval-Guided Topic Adaptation for Test-Time Detection of Fake News Videos](https)

**Jian Lang**, Rongpei Hong, Ting Zhong, Yong Wang, Fan Zhou‚Ä†

<!-- **CCF A** \|  -->

[**Github**](https://github.com/Jian-Lang/RADAR) 

- RADAR is the first work to achieves the test-time adaptation of the Fake News Video Detection, enabling fast adaptation of obselete models to evolving news videos with shifting topic distributions in the dynamic world.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div class="badge">KDD 2026</div><div class="badge">CCF A</div><img src='images/alarm.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement](https) 

**Jian Lang**, Rongpei Hong, Ting Zhong, Leiting Chen, Qiang Gao, Fan Zhou‚Ä†

<!-- **CCF A** \| [**Project**](https://github.com/Jian-Lang/ALARM)  -->

[**Project**](https://github.com/Jian-Lang/ALARM) 

- ALARM is the first label-free harmful meme detection framework powered by Large Multimodal Model self-improvement, which mitigates label scarcity and enables prompt and robust adaptation to evolving harmful content in web memes.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div class="badge">AAAI 2026</div><img src='images/scanner.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Shedding the Facades, Connecting the Domains: Detecting Shifting Multimodal Hate Video with Test-Time Adaptation](https)

Jiao Li, **Jian Lang**, Xikai Tang‚Ä†, Ting Zhong, Fan Zhou

**CCF A** \| [**Project**](https://github.com/) 

- SCANNER is the first test-time adaptation framework tailored for distribution shifting hate video detection.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div class="badge">KDD 2025</div><img src='images/redeem.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[REDEEMing Modality Information Loss: Retrieval-Guided Conditional Generation for Severely Modality Missing Learning](https)

**Jian Lang**, Rongpei Hong, Zhangtao Cheng, Ting Zhong, Fan Zhou‚Ä†

**CCF A** \| [**Project**](https://github.com/Jian-Lang/REDEEM) 

- REDEEM, the extension work of our RAGPT accetped to AAAI 2025, is a novel framework that pioneers a retrieval-guided conditional generation paradigm for enhancing the missing robustness of pre-trained Multimodal Transformer.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div class="badge">AAAI 2025</div><img src='images/ragpt.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Retrieval-Augmented Dynamic Prompt Tuning for Incomplete Multimodal Learning](https), **Jian Lang**\*, Zhangtao Cheng\*, Ting Zhong, Fan Zhou‚Ä†

**AAAI 2025** \| [**Project**](https://github.com/Jian-Lang/RAGPT) 

- RAGPT is a novel retrieval-augmented dynamic prompt-tuning framework for enhancing the robustness of pre-trained Multimodal Transformer under modality missing conditions
</div>
</div>



<!-- <div class='paper-box'><div class='paper-box-image'><img src='images/fs.png' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[FastSpeech: Fast, Robust and Controllable Text to Speech](https://papers.nips.cc/paper/8580-fastspeech-fast-robust-and-controllable-text-to-speech.pdf), **Yi Ren**, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu

**NeurIPS 2019** \| [**Project**](https://speechresearch.github.io/fastspeech/) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>

- FastSpeech is the first fully parallel end-to-end speech synthesis model.
- **Academic Impact**: This work is included by many famous speech synthesis open-source projects, such as [ESPNet ![](https://img.shields.io/github/stars/espnet/espnet?style=social)](https://github.com/espnet/espnet). Our work are promoted by more than 20 media and forums, such as [Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s/UkFadiUBy-Ymn-zhJ95JcQ)„ÄÅ[InfoQ](https://www.infoq.cn/article/tvy7hnin8bjvlm6g0myu).
- **Industry Impact**: FastSpeech has been deployed in [Microsoft Azure TTS service](https://techcommunity.microsoft.com/t5/azure-ai/neural-text-to-speech-extends-support-to-15-more-languages-with/ba-p/1505911) and supports 49 more languages with state-of-the-art AI quality. It was also shown as a text-to-speech system acceleration example in [NVIDIA GTC2020](https://resources.nvidia.com/events/GTC2020s21420).
</div>
</div> -->

## Video Analysis & Detection

<!-- <div class='paper-box'><div class='paper-box-image'><img src='images/radar.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Nipping Rumors in the Bud: Retrieval-Guided Topic Adaptation for Test-Time Detection of Fake News Videos](https), **Jian Lang**, Rongpei Hong, Ting Zhong, Yong Wang, Fan Zhou‚Ä†

**KDD 2026** \| [**Project**](https://github.com/Jian-Lang/RADAR) 

- RADAR is the first work to achieves the test-time adaptation of the Fake News Video Detection, enabling fast adaptation of obselete models to evolving news videos with shifting topic distributions in the dynamic world.
</div>
</div> -->




<!-- <div class='paper-box'><div class='paper-box-image'><img src='images/scanner.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Shedding the Facades, Connecting the Domains: Detecting Shifting Multimodal Hate Video with Test-Time Adaptation](https), Jiao Li, **Jian Lang**, Xikai Tang‚Ä†, Ting Zhong, Fan Zhou

**AAAI 2026** \| [**Project**](https://github.com/Jolieresearch/SCANNER) 

- SCANNER is the first test-time adaptation framework tailored for distribution shifting hate video detection.
</div>
</div>

 -->
<div class='paper-box'><div class='paper-box-image'> <div class="badge">ICCV 2025</div><img src='images/crave.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">
[Borrowing Eyes for the Blind Spot: Overcoming Data Scarcity in Malicious Video Detection via Cross-Domain Retrieval Augmentation](https), Rongpei Hong\*, **Jian Lang**\*, Ting Zhong, Fan Zhou‚Ä†

**ICCV 2025** \| [**Project**](https://github.com)

- CRAVE is a novel cross-domain retrieval augmentation framework that transfers knowledge from resource-rich image-text domain to enhance malicious video detection.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'> <div class="badge">WWW 2025</div><img src='images/more.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Biting Off More Than You Can Detect: Retrieval-Augmented Multimodal Experts for Short Video Hate Detection](https), **Jian Lang**, Rongpei Hong, Jin Xu, Xovee Xu, Yili Li, Fan Zhou‚Ä†


**WWW 2025** \| [**Project**](https://github.com/Jian-Lang/MoRE) 

- MoRE is a novel mixture of retrieval-augmented multimodal experts framework to enhance hate video detection.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div class="badge">WWW 2025</div><img src='images/exmrd.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Following Clues, Approaching the Truth: Explainable Micro-Video Rumor Detection via Chain-of-Thought Reasoning](https), Rongpei Hong, **Jian Lang**, Jin Xu, Zhangtao Cheng, Ting Zhong‚Ä†, Fan Zhou

**WWW 2025** \| [**Project**](https://github.com) 

- ExMRD is the first explainable fake news video detection framework powered by the Chain-of-Thought Reasoning.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div class="badge">ICME 2025</div><img src='images/real.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[REAL: Retrieval-Augmented Prototype Alignment for Improved Fake News Video Detection](https), Yili Li, **Jian Lang**, Rongpei Hong, Qing Chen, Zhangtao Cheng, Jia Chen, Ting Zhong, Fan Zhou‚Ä†

**ICME 2025** \| [**Project**](https://github.com/Jian-Lang/REAL) 

- REAL is a novel model-agnostic framework that generates manipulation-aware representations to enhance existing methods in detecting fake news videos with only subtle modifications to the original authentic ones.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div class="badge">SIGIR 2024</div><img src='images/mmra.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[Predicting Micro-video Popularity via Multi-modal Retrieval Augmentation](https), Ting Zhong, **Jian Lang**, Yifan Zhang, Zhangtao Cheng, Kunpeng Zhang, Fan Zhou‚Ä†

**SIGIR 2024** \| [**Project**](https://github.com/UESTC-ICDM/MMRA) 

- MMRA is a multi-modal retrieval-augmented popularity prediction model that enhances prediction accuracy using relevant retrieved information.
</div>
</div>



## Large Multimodal Model


<!-- <div class='paper-box'><div class='paper-box-image'><img src='images/alarm.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement](https), **Jian Lang**, Rongpei Hong, Ting Zhong, Leiting Chen, Qiang Gao, Fan Zhou‚Ä†

**KDD 2026** \| [**Project**](https://github.com/Jian-Lang/ALARM) 

- ALARM is the first label-free harmful meme detection framework powered by Large Multimodal Model self-improvement, which mitigates label scarcity and enables prompt and robust adaptation to evolving harmful content in web memes.
</div>
</div> -->


<div class='paper-box'><div class='paper-box-image'><div class="badge">KDD 2026</div><img src='images/tame.jpg' alt="sym" width="100%"></div>
<div class='paper-box-text' markdown="1">

[TAMEing Long Contexts in Personalization: Towards Training-Free and State-Aware MLLM Personalized Assistant](https), Rongpei Hong, **Jian Lang**, Ting Zhong‚Ä†, Yong Wang, Fan Zhou

**KDD 2026** \| [**Project**](https://github.com) 

- TAME is the first training-free and state-aware personalized Multimodal Large Multimodal Model assistant powered by double memories.
</div>
</div>


# üéñ Honors and Awards
- *2021.10* Tencent Scholarship (Top 1%)
- *2021.10* National Scholarship (Top 1%)
- *2020.12* [Baidu Scholarship](https://baike.baidu.com/item/%E7%99%BE%E5%BA%A6%E5%A5%96%E5%AD%A6%E9%87%91/9929412) (10 students in the world each year)
- *2020.12* [AI Chinese new stars](https://mp.weixin.qq.com/s?__biz=MzA4NzQ5MTA2NA==&mid=2653639431&idx=1&sn=25b6368c1954419b9090840347d9a27d&chksm=8be75b90bc90d286a5af3ef8e610e822d705dc3cf4382b45e3f14489f3e7ec4fd8c95ed0eceb&mpshare=1&scene=2&srcid=0511LMlj9Qv9DeIZAjMjYAU9&sharer_sharetime=1620731348139&sharer_shareid=631c113940cb81f34895aa25ab14422a#rd) (100 worldwide each year)
- *2020.12* [AI Chinese New Star Outstanding Scholar](https://mp.weixin.qq.com/s?__biz=MzA4NzQ5MTA2NA==&mid=2653639431&idx=1&sn=25b6368c1954419b9090840347d9a27d&chksm=8be75b90bc90d286a5af3ef8e610e822d705dc3cf4382b45e3f14489f3e7ec4fd8c95ed0eceb&mpshare=1&scene=2&srcid=0511LMlj9Qv9DeIZAjMjYAU9&sharer_sharetime=1620731348139&sharer_shareid=631c113940cb81f34895aa25ab14422a#rd) (10 candidates worldwide each year)
- *2020.12* ByteDance Scholars Program (10 students in China each year)
- *2020.10* Tianzhou Chen Scholarship (Top 1%)
- *2020.10* National Scholarship (Top 1%)
- *2015.10* National Scholarship (Undergraduate) (Top 1%)

# üìñ Educations
- *2019.06 - 2022.04 (now)*, Master, Zhejiang Univeristy, Hangzhou.
- *2015.09 - 2019.06*, Undergraduate, Chu Kochen Honors College, Zhejiang Univeristy, Hangzhou.
- *2012.09 - 2015.06*, Luqiao Middle School, Taizhou.

# üí¨ Invited Talks
- *2021.06*, Audio & Speech Synthesis, Huawei internal talk
- *2021.03*, Non-autoregressive Speech Synthesis, PaperWeekly & biendata \| [\[video\]](https://www.bilibili.com/video/BV1uf4y1t7Hr/)
- *2020.12*, Non-autoregressive Speech Synthesis, Huawei Noah's Ark Lab internal talk

# üíª Internships
- *2019.05 - 2020.02*, [EnjoyMusic](https://enjoymusic.ai/), Hangzhou.
- *2019.02 - 2019.05*, [YiWise](https://www.yiwise.com/), Hangzhou.
- *2018.08 - 2019.02*, [MSRA, machine learning Group](https://www.microsoft.com/en-us/research/group/machine-learning-research-group/), Beijing.
- *2018.01 - 2018.06*, [NetEase, AI department](https://hr.163.com/zc/12-ai/index.html), Hangzhou.
- *2017.08 - 2018.12*, DashBase (acquired by [Cisco](https://blogs.cisco.com/news/349511)), Hangzhou.